---
title: "Using Machine Learning to Quantify  Weight Lifting Exercise"
author: "paperless247"
date: "Friday, April 24, 2015"
output: html_document
---


R version 3.1.2 (2014-10-31) -- "Pumpkin Helmet"

Platform: i386-w64-mingw32/i386 (32-bit)

RStudio Version 0.98.1103


## Introduction

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement --- a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it.

In this project, my goal is to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset). "

Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).


## Data Sources

The training and testing data for this project are available here: 

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The data for this project comes from this original source: http://groupware.les.inf.puc-rio.br/har


## Objective

The goal of my project is to predict the manner,`classe` variable, in which they did the exercise.

I will use my best prediction model to predict 20 different test cases. 


## Setting Working Environment

The following libraries are needed for analysis
```{r, warning=FALSE, cache=TRUE}
library(caret)
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(rattle)
library(randomForest)
```

Set seed
```{r}
set.seed(12345)
```


## Getting and Partioning the Data

Load the training and testing data sets
```{r, cache=TRUE}
trainUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

training <- read.csv(url(trainUrl), na.strings=c("NA","#DIV/0!",""))
testing <- read.csv(url(testUrl), na.strings=c("NA","#DIV/0!",""))
```


Partioning training data set into two data sets: 60% for myTraining and 40% for myTesting
```{r, cache=TRUE}
inTrain <- createDataPartition(y=training$classe, p=0.6, list=FALSE)
myTraining <- training[inTrain, ]
myTesting <- training[-inTrain, ]
dim(myTraining)
dim(myTesting)
```


## Cleaning the data

### Step 1: Cleaning NearZeroVariance Variables (NZV)
```{r, cache=TRUE}
myDataNZV <- nearZeroVar(myTraining, saveMetrics=TRUE)
dim(myDataNZV)
```

I pick more variables base on their freqRatio, percentUnique, and nzv. I run the following code to create another subset without NZV variables:

```{r, cache=TRUE}
myNZVvars <- names(myTraining) %in% c("new_window", "kurtosis_roll_belt", "kurtosis_picth_belt",
"kurtosis_yaw_belt", "skewness_roll_belt", "skewness_roll_belt.1", "skewness_yaw_belt",
"max_yaw_belt", "min_yaw_belt", "amplitude_yaw_belt", "avg_roll_arm", "stddev_roll_arm",
"var_roll_arm", "avg_pitch_arm", "stddev_pitch_arm", "var_pitch_arm", "avg_yaw_arm",
"stddev_yaw_arm", "var_yaw_arm", "kurtosis_roll_arm", "kurtosis_picth_arm",
"kurtosis_yaw_arm", "skewness_roll_arm", "skewness_pitch_arm", "skewness_yaw_arm",
"max_roll_arm", "min_roll_arm", "min_pitch_arm", "amplitude_roll_arm", "amplitude_pitch_arm",
"kurtosis_roll_dumbbell", "kurtosis_picth_dumbbell", "kurtosis_yaw_dumbbell", "skewness_roll_dumbbell",
"skewness_pitch_dumbbell", "skewness_yaw_dumbbell", "max_yaw_dumbbell", "min_yaw_dumbbell",
"amplitude_yaw_dumbbell", "kurtosis_roll_forearm", "kurtosis_picth_forearm", "kurtosis_yaw_forearm",
"skewness_roll_forearm", "skewness_pitch_forearm", "skewness_yaw_forearm", "max_roll_forearm",
"max_yaw_forearm", "min_roll_forearm", "min_yaw_forearm", "amplitude_roll_forearm",
"amplitude_yaw_forearm", "avg_roll_forearm", "stddev_roll_forearm", "var_roll_forearm",
"avg_pitch_forearm", "stddev_pitch_forearm", "var_pitch_forearm", "avg_yaw_forearm",
"stddev_yaw_forearm", "var_yaw_forearm")
myTraining <- myTraining[!myNZVvars]
dim(myTraining)
```


### Step 2: Removing the variable ID

```{r, cache=TRUE}
myTraining <- myTraining[c(-1)]
```

### Step 3: Cleaning Variables with many NAs.

I use a 60% threshold of NA's

```{r, cache=TRUE}

trainingV3 <- myTraining
for(i in 1:length(myTraining)) {
  if(sum(is.na( myTraining[,i]))/nrow(myTraining) >= .6 ) {
    for(j in 1:length(trainingV3)) {
      if( length( grep(names(myTraining[i]), names(trainingV3)[j])) ==1)  {
				trainingV3 <- trainingV3[,-j]
			}	
		} 
	}
}
dim(trainingV3)
myTraining <- trainingV3
rm(trainingV3)
```


### Transform myTesting dataset with the same above steps

```{r, cache=TRUE}
step1 <- colnames(myTraining)
step2 <- colnames(myTraining[, -58]) #already with classe column removed
myTesting <- myTesting[step1]
testing <- testing[step2]

dim(myTesting)
dim(testing)
```

I check and convert the variables in the `testing` data set to the ones in `myTraining` data set

```{r, cache=TRUE}
for (i in 1:length(testing) ) {
        for(j in 1:length(myTraining)) {
		if( length( grep(names(myTraining[i]), names(testing)[j]) ) ==1)  {
			class(testing[j]) <- class(myTraining[i])
		}      
	}      
}

testing <- rbind(myTraining[2, -58] , testing)
testing <- testing[-1,] # remove the "user_name" variable
```


## Decision Tree (DT) Algorithm

Creating DT model:
```{r, cache=TRUE}
modFit_DT <- rpart(classe ~ ., data=myTraining, method="class")
fancyRpartPlot(modFit_DT)
```

Predicting with the DT model:
```{r, cache=TRUE}
predictions_DT <- predict(modFit_DT, myTesting, type = "class")
```

Test results:
```{r, cache=TRUE}
DT <- confusionMatrix(predictions_DT, myTesting$classe)
DT
```

The overall accuracy of this model is 0.8789

## Random Forests Algorithm

```{r, cache=TRUE}
modFit_RF <- randomForest(classe ~. , data=myTraining)
```

Predicting:
```{r, cache=TRUE}
predictions_RF <- predict(modFit_RF, myTesting, type = "class")
```

Test results:
```{r, cache=TRUE}
RF <- confusionMatrix(predictions_RF, myTesting$classe)
RF
```
Random Forests predict better results. The overall accuracy of this model is 0.9986


### Error estimation with cross validation for RF

Using the model that I have trained, the out of error rate is expected to be less than 1%, as the accuracy of the model observed above is 99.86%.


## Submit Answers for the Assignment

With the Test data set, I use the Random Forests model to predict the results
```{r, cache=TRUE}
predictions <- predict(modFit_RF, testing, type = "class")
```

Function to generate files with predictions to submit for assignment
```{r, cache=TRUE}

pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(predictions)
```


## Conclusion

I can use Random Forests algorithm to create a very high accuracy model comparing to  Decision Tree algorithm.

## Thank you for reading and grading!